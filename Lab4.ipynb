{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386047da",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4  \n",
    "## Проведение исследований со случайным лесом (Random Forest)\n",
    "\n",
    "В лабораторной работе исследуется алгоритм **случайного леса**:\n",
    "- **RandomForestClassifier** для задачи классификации (loan_status);\n",
    "- **RandomForestRegressor** для задачи регрессии (song_popularity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63f0ac",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63759637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    root_mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6a878",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных и формирование выборок\n",
    "\n",
    "- отделяется целевая переменная;\n",
    "- удаляются идентификаторы (`customer_id`, `song_name`);\n",
    "- задаются списки категориальных и числовых признаков;\n",
    "- выполняется разбиение на train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd67e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: (40000, 18) (10000, 18)\n",
      "Regression: (15068, 13) (3767, 13)\n"
     ]
    }
   ],
   "source": [
    "# Пути к данным\n",
    "LOAN_PATH = os.path.join(\"data\", \"Loan_approval_data_2025.csv\")\n",
    "SONG_PATH = os.path.join(\"data\", \"song_data.csv\")\n",
    "\n",
    "# Загрузка датасетов\n",
    "loan_df = pd.read_csv(LOAN_PATH)\n",
    "song_df = pd.read_csv(SONG_PATH)\n",
    "\n",
    "# --- Classification ---\n",
    "y_cls = loan_df[\"loan_status\"].astype(int)\n",
    "X_cls = loan_df.drop(columns=[\"loan_status\"])\n",
    "\n",
    "if \"customer_id\" in X_cls.columns:\n",
    "    X_cls = X_cls.drop(columns=[\"customer_id\"])\n",
    "\n",
    "cat_cols_cls = X_cls.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_cls = X_cls.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=RANDOM_STATE, stratify=y_cls\n",
    ")\n",
    "\n",
    "# --- Regression ---\n",
    "y_reg = song_df[\"song_popularity\"].astype(float)\n",
    "X_reg = song_df.drop(columns=[\"song_popularity\"])\n",
    "\n",
    "if \"song_name\" in X_reg.columns:\n",
    "    X_reg = X_reg.drop(columns=[\"song_name\"])\n",
    "\n",
    "cat_cols_reg = X_reg.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_reg = X_reg.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Classification:\", Xc_train.shape, Xc_test.shape)\n",
    "print(\"Regression:\", Xr_train.shape, Xr_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e3d00",
   "metadata": {},
   "source": [
    "## 2. Бейзлайн и оценка качества (Random Forest)\n",
    "\n",
    "Случайный лес не требует масштабирования признаков.\n",
    "Для категориальных переменных выполняется one-hot кодирование.\n",
    "\n",
    "Бейзлайн:\n",
    "- RandomForestClassifier с параметрами по умолчанию (фиксируется random_state);\n",
    "- RandomForestRegressor с параметрами по умолчанию (фиксируется random_state).\n",
    "\n",
    "Далее рассчитываются метрики качества:\n",
    "- классификация: Accuracy, Precision, Recall, F1, ROC-AUC;\n",
    "- регрессия: RMSE, MAE, R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde7ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classification: {'accuracy': 0.9122, 'precision': 0.9161719733765066, 'recall': 0.9251589464123524, 'f1': 0.9206435285610991, 'roc_auc': np.float64(0.9747151290312477)}\n",
      "Baseline regression: {'rmse': 17.20353452475776, 'mae': 12.298149638534817, 'r2': 0.3860694795460795}\n"
     ]
    }
   ],
   "source": [
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "prep_cls = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols_cls),\n",
    "    (\"cat\", cat_pipe, cat_cols_cls)\n",
    "])\n",
    "\n",
    "prep_reg = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols_reg),\n",
    "    (\"cat\", cat_pipe, cat_cols_reg)\n",
    "])\n",
    "\n",
    "baseline_cls = Pipeline([\n",
    "    (\"prep\", prep_cls),\n",
    "    (\"model\", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "baseline_reg = Pipeline([\n",
    "    (\"prep\", prep_reg),\n",
    "    (\"model\", RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "baseline_cls.fit(Xc_train, yc_train)\n",
    "baseline_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "yc_pred = baseline_cls.predict(Xc_test)\n",
    "yc_proba = baseline_cls.predict_proba(Xc_test)[:, 1]\n",
    "\n",
    "yr_pred = baseline_reg.predict(Xr_test)\n",
    "\n",
    "baseline_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_test, yc_pred),\n",
    "    \"precision\": precision_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"recall\": recall_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"f1\": f1_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_test, yc_proba)\n",
    "}\n",
    "\n",
    "baseline_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_test, yr_pred),\n",
    "    \"mae\": mean_absolute_error(yr_test, yr_pred),\n",
    "    \"r2\": r2_score(yr_test, yr_pred)\n",
    "}\n",
    "\n",
    "print(\"Baseline classification:\", baseline_cls_res)\n",
    "print(\"Baseline regression:\", baseline_reg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9247826",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна: гипотезы и проверка\n",
    "\n",
    "### Гипотезы для классификации\n",
    "1) Подбор `n_estimators`, `max_depth`, `min_samples_leaf` улучшит качество (особенно ROC-AUC).  \n",
    "2) `class_weight='balanced'` может помочь при возможном дисбалансе классов.  \n",
    "3) Подбор `max_features` влияет на разнообразие деревьев → меняет качество и переобучение.\n",
    "\n",
    "### Гипотезы для регрессии\n",
    "1) Подбор `n_estimators`, `max_depth`, `min_samples_leaf` снизит RMSE.  \n",
    "2) `max_features` влияет на смещение/дисперсию ансамбля и может улучшить обобщение.\n",
    "\n",
    "Ниже проводится GridSearchCV:\n",
    "- классификация: оптимизация по ROC-AUC;\n",
    "- регрессия: оптимизация по neg-RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9495a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cls params: {'model__class_weight': None, 'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 10, 'model__n_estimators': 200}\n",
      "Best CV ROC-AUC: 0.9749121037240706\n",
      "Improved classification: {'accuracy': 0.9127, 'precision': 0.9166966534724721, 'recall': 0.9255222524977293, 'f1': 0.9210883123926602, 'roc_auc': np.float64(0.9747544703520615)}\n",
      "Best reg params: {'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 200}\n",
      "Best CV -RMSE: -17.35700555055774\n",
      "Improved regression: {'rmse': 17.04182545744198, 'mae': 12.105365241972857, 'r2': 0.3975568325074823}\n"
     ]
    }
   ],
   "source": [
    "# --- Tuning: Classification ---\n",
    "param_grid_cls = {\n",
    "    \"model__n_estimators\": [200],\n",
    "    \"model__max_depth\": [None, 10],\n",
    "    \"model__min_samples_split\": [2, 10],\n",
    "    \"model__min_samples_leaf\": [1, 5],\n",
    "    \"model__max_features\": [\"sqrt\", None],\n",
    "    \"model__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "tuned_cls = Pipeline([\n",
    "    (\"prep\", prep_cls),\n",
    "    (\"model\", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=1))\n",
    "])\n",
    "\n",
    "cv_cls = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs_cls = GridSearchCV(\n",
    "    tuned_cls,\n",
    "    param_grid=param_grid_cls,\n",
    "    cv=cv_cls,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_cls.fit(Xc_train, yc_train)\n",
    "\n",
    "best_cls = gs_cls.best_estimator_\n",
    "print(\"Best cls params:\", gs_cls.best_params_)\n",
    "print(\"Best CV ROC-AUC:\", gs_cls.best_score_)\n",
    "\n",
    "yc_pred2 = best_cls.predict(Xc_test)\n",
    "yc_proba2 = best_cls.predict_proba(Xc_test)[:, 1]\n",
    "\n",
    "improved_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_test, yc_pred2),\n",
    "    \"precision\": precision_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"recall\": recall_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"f1\": f1_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_test, yc_proba2)\n",
    "}\n",
    "print(\"Improved classification:\", improved_cls_res)\n",
    "\n",
    "# --- Tuning: Regression ---\n",
    "param_grid_reg = {\n",
    "    \"model__n_estimators\": [200],\n",
    "    \"model__max_depth\": [None, 10],\n",
    "    \"model__min_samples_split\": [2, 10],\n",
    "    \"model__min_samples_leaf\": [1, 5],\n",
    "    \"model__max_features\": [\"sqrt\", None]\n",
    "}\n",
    "\n",
    "tuned_reg = Pipeline([\n",
    "    (\"prep\", prep_reg),\n",
    "    (\"model\", RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "cv_reg = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs_reg = GridSearchCV(\n",
    "    tuned_reg,\n",
    "    param_grid=param_grid_reg,\n",
    "    cv=cv_reg,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "best_reg = gs_reg.best_estimator_\n",
    "print(\"Best reg params:\", gs_reg.best_params_)\n",
    "print(\"Best CV -RMSE:\", gs_reg.best_score_)\n",
    "\n",
    "yr_pred2 = best_reg.predict(Xr_test)\n",
    "\n",
    "improved_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_test, yr_pred2),\n",
    "    \"mae\": mean_absolute_error(yr_test, yr_pred2),\n",
    "    \"r2\": r2_score(yr_test, yr_pred2)\n",
    "}\n",
    "print(\"Improved regression:\", improved_reg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf7601-7b95-4e1a-bc3e-b567e51065b0",
   "metadata": {},
   "source": [
    "## 4. Имплементация случайного леса\n",
    "\n",
    "Реализуется упрощённый случайный лес:\n",
    "- базовое дерево (самописное) + bootstrap выборка для каждого дерева;\n",
    "- усреднение предсказаний (для классификации — средняя вероятность, для регрессии — среднее значение).\n",
    "\n",
    "Из-за вычислительной сложности мой лес обучается на подвыборке данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7707f3b-6a5d-45f5-8c1b-05e76f996fad",
   "metadata": {},
   "source": [
    "## 5. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c05613a1-27e7-474e-80fc-5cef6b93e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS: (40000, 27) (10000, 27)\n",
      "REG: (15068, 13) (3767, 13)\n",
      "Subsample CLS: (8000, 27) Subsample REG: (8000, 13)\n"
     ]
    }
   ],
   "source": [
    "def to_dense(X):\n",
    "    return X.toarray() if sparse.issparse(X) else np.asarray(X)\n",
    "\n",
    "def sample_indices(n, max_n=8000, random_state=42):\n",
    "    if n <= max_n:\n",
    "        return np.arange(n)\n",
    "    r = np.random.default_rng(random_state)\n",
    "    return r.choice(n, size=max_n, replace=False)\n",
    "\n",
    "# Препроцессинг тот же, что и для sklearn\n",
    "Xc_tr = prep_cls.fit_transform(Xc_train)\n",
    "Xc_te = prep_cls.transform(Xc_test)\n",
    "yc_tr = np.asarray(yc_train).astype(int)\n",
    "yc_te = np.asarray(yc_test).astype(int)\n",
    "\n",
    "Xr_tr = prep_reg.fit_transform(Xr_train)\n",
    "Xr_te = prep_reg.transform(Xr_test)\n",
    "yr_tr = np.asarray(yr_train).astype(float)\n",
    "yr_te = np.asarray(yr_test).astype(float)\n",
    "\n",
    "# Для самописных деревьев/леса лучше работать с dense\n",
    "Xc_tr_arr, Xc_te_arr = to_dense(Xc_tr), to_dense(Xc_te)\n",
    "Xr_tr_arr, Xr_te_arr = to_dense(Xr_tr), to_dense(Xr_te)\n",
    "\n",
    "print(\"CLS:\", Xc_tr_arr.shape, Xc_te_arr.shape)\n",
    "print(\"REG:\", Xr_tr_arr.shape, Xr_te_arr.shape)\n",
    "\n",
    "# Подвыборка для ускорения\n",
    "idx_c = sample_indices(Xc_tr_arr.shape[0], max_n=8000, random_state=RANDOM_STATE)\n",
    "idx_r = sample_indices(Xr_tr_arr.shape[0], max_n=8000, random_state=RANDOM_STATE)\n",
    "\n",
    "Xc_tr_s, yc_tr_s = Xc_tr_arr[idx_c], yc_tr[idx_c]\n",
    "Xr_tr_s, yr_tr_s = Xr_tr_arr[idx_r], yr_tr[idx_r]\n",
    "\n",
    "print(\"Subsample CLS:\", Xc_tr_s.shape, \"Subsample REG:\", Xr_tr_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabe026-6e72-42e9-8b89-31a4dc1c567d",
   "metadata": {},
   "source": [
    "## 6. Имплементация дерева + классификация/регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd161424-0b34-4593-80a9-13df56a8c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom trees defined.\n"
     ]
    }
   ],
   "source": [
    "class _Node:\n",
    "    __slots__ = (\"feature\", \"threshold\", \"left\", \"right\", \"value\")\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "class MyDecisionTreeBase:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                 max_features=None, random_state=42):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = int(min_samples_split)\n",
    "        self.min_samples_leaf = int(min_samples_leaf)\n",
    "        self.max_features = max_features\n",
    "        self.random_state = int(random_state)\n",
    "        self.rng_ = np.random.default_rng(self.random_state)\n",
    "        self.root_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "    def _feature_subset(self, n_features):\n",
    "        if self.max_features is None:\n",
    "            return np.arange(n_features)\n",
    "        if self.max_features == \"sqrt\":\n",
    "            k = max(1, int(np.sqrt(n_features)))\n",
    "        elif self.max_features == \"log2\":\n",
    "            k = max(1, int(np.log2(n_features)))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            k = max(1, min(n_features, self.max_features))\n",
    "        else:\n",
    "            k = n_features\n",
    "        return self.rng_.choice(n_features, size=k, replace=False)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.root_ = self._build(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build(self, X, y, depth):\n",
    "        n = X.shape[0]\n",
    "        if n < self.min_samples_split:\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "        if self._stop(y):\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "\n",
    "        feat_ids = self._feature_subset(self.n_features_)\n",
    "        best = self._best_split(X, y, feat_ids)\n",
    "        if best is None:\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "\n",
    "        f, thr = best\n",
    "        mask = X[:, f] <= thr\n",
    "        if mask.sum() < self.min_samples_leaf or (~mask).sum() < self.min_samples_leaf:\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "\n",
    "        left = self._build(X[mask], y[mask], depth+1)\n",
    "        right = self._build(X[~mask], y[~mask], depth+1)\n",
    "        return _Node(feature=f, threshold=thr, left=left, right=right, value=None)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        out = np.empty(X.shape[0], dtype=float)\n",
    "        for i in range(X.shape[0]):\n",
    "            out[i] = self._predict_one(X[i], self.root_)\n",
    "        return out\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        while node.value is None:\n",
    "            node = node.left if x[node.feature] <= node.threshold else node.right\n",
    "        return node.value\n",
    "\n",
    "    # переопределяются\n",
    "    def _leaf_value(self, y): raise NotImplementedError\n",
    "    def _stop(self, y): raise NotImplementedError\n",
    "    def _best_split(self, X, y, feat_ids): raise NotImplementedError\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier(MyDecisionTreeBase):\n",
    "    def _leaf_value(self, y):\n",
    "        vals, cnts = np.unique(y, return_counts=True)\n",
    "        return int(vals[np.argmax(cnts)])\n",
    "\n",
    "    def _stop(self, y):\n",
    "        return np.unique(y).size == 1\n",
    "\n",
    "    def _gini(self, y):\n",
    "        _, cnts = np.unique(y, return_counts=True)\n",
    "        p = cnts / cnts.sum()\n",
    "        return 1.0 - np.sum(p*p)\n",
    "\n",
    "    def _best_split(self, X, y, feat_ids):\n",
    "        base = self._gini(y)\n",
    "        best_gain, best = 0.0, None\n",
    "\n",
    "        for f in feat_ids:\n",
    "            col = X[:, f]\n",
    "            uniq = np.unique(col)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "\n",
    "            # ограничим число порогов\n",
    "            if uniq.size > 40:\n",
    "                thr_list = np.quantile(uniq, np.linspace(0.05, 0.95, 40))\n",
    "            else:\n",
    "                thr_list = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "\n",
    "            for thr in thr_list:\n",
    "                mask = col <= thr\n",
    "                if mask.sum() < self.min_samples_leaf or (~mask).sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                g = mask.mean()*self._gini(y[mask]) + (1-mask.mean())*self._gini(y[~mask])\n",
    "                gain = base - g\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best = gain, (f, float(thr))\n",
    "        return best\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = self.predict(X).astype(int)\n",
    "        p1 = pred.astype(float)\n",
    "        p0 = 1.0 - p1\n",
    "        return np.vstack([p0, p1]).T\n",
    "\n",
    "\n",
    "class MyDecisionTreeRegressor(MyDecisionTreeBase):\n",
    "    def _leaf_value(self, y):\n",
    "        return float(np.mean(y))\n",
    "\n",
    "    def _stop(self, y):\n",
    "        return float(np.var(y)) < 1e-12\n",
    "\n",
    "    def _mse(self, y):\n",
    "        if y.size == 0:\n",
    "            return 0.0\n",
    "        m = float(np.mean(y))\n",
    "        return float(np.mean((y - m)**2))\n",
    "\n",
    "    def _best_split(self, X, y, feat_ids):\n",
    "        base = self._mse(y)\n",
    "        best_gain, best = 0.0, None\n",
    "\n",
    "        for f in feat_ids:\n",
    "            col = X[:, f]\n",
    "            uniq = np.unique(col)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "\n",
    "            if uniq.size > 40:\n",
    "                thr_list = np.quantile(uniq, np.linspace(0.05, 0.95, 40))\n",
    "            else:\n",
    "                thr_list = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "\n",
    "            for thr in thr_list:\n",
    "                mask = col <= thr\n",
    "                if mask.sum() < self.min_samples_leaf or (~mask).sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                mse_split = mask.mean()*self._mse(y[mask]) + (1-mask.mean())*self._mse(y[~mask])\n",
    "                gain = base - mse_split\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best = gain, (f, float(thr))\n",
    "        return best\n",
    "\n",
    "print(\"Custom trees defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf99d4-13a3-4133-ac8e-a59ac66353d3",
   "metadata": {},
   "source": [
    "## 7. Имплементация RandomForest (классификация/регрессия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ced4e6e5-c2b3-40d4-aa52-26b59381ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom random forests defined.\n"
     ]
    }
   ],
   "source": [
    "class MyRandomForestClassifier:\n",
    "    def __init__(self, n_estimators=30, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                 max_features=\"sqrt\", bootstrap=True, random_state=42):\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = int(min_samples_split)\n",
    "        self.min_samples_leaf = int(min_samples_leaf)\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bool(bootstrap)\n",
    "        self.random_state = int(random_state)\n",
    "        self.rng_ = np.random.default_rng(self.random_state)\n",
    "        self.trees_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y).astype(int)\n",
    "        n = X.shape[0]\n",
    "        self.trees_ = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                idx = self.rng_.integers(0, n, size=n)\n",
    "            else:\n",
    "                idx = np.arange(n)\n",
    "\n",
    "            tree = MyDecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features,\n",
    "                random_state=self.rng_.integers(0, 10**9)\n",
    "            )\n",
    "            tree.fit(X[idx], y[idx])\n",
    "            self.trees_.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        proba_sum = np.zeros((X.shape[0], 2), dtype=float)\n",
    "        for t in self.trees_:\n",
    "            proba_sum += t.predict_proba(X)\n",
    "        return proba_sum / len(self.trees_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "class MyRandomForestRegressor:\n",
    "    def __init__(self, n_estimators=30, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                 max_features=\"sqrt\", bootstrap=True, random_state=42):\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = int(min_samples_split)\n",
    "        self.min_samples_leaf = int(min_samples_leaf)\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bool(bootstrap)\n",
    "        self.random_state = int(random_state)\n",
    "        self.rng_ = np.random.default_rng(self.random_state)\n",
    "        self.trees_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y).astype(float)\n",
    "        n = X.shape[0]\n",
    "        self.trees_ = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                idx = self.rng_.integers(0, n, size=n)\n",
    "            else:\n",
    "                idx = np.arange(n)\n",
    "\n",
    "            tree = MyDecisionTreeRegressor(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features,\n",
    "                random_state=self.rng_.integers(0, 10**9)\n",
    "            )\n",
    "            tree.fit(X[idx], y[idx])\n",
    "            self.trees_.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        preds = np.zeros((X.shape[0], len(self.trees_)), dtype=float)\n",
    "        for i, t in enumerate(self.trees_):\n",
    "            preds[:, i] = t.predict(X)\n",
    "        return preds.mean(axis=1)\n",
    "\n",
    "print(\"Custom random forests defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204aca9-d855-44ed-a38c-e3cd99dd3f3e",
   "metadata": {},
   "source": [
    "## 8. Обучение/оценка самописного леса (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2926dbcc-ceb4-4416-9b18-901d608a7fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRF baseline (classification): {'accuracy': 0.8919, 'precision': 0.8916430594900849, 'recall': 0.9148047229791099, 'f1': 0.903075405720434, 'roc_auc': np.float64(0.9624247145127445)}\n",
      "MyRF baseline (regression):    {'rmse': 18.606115372680954, 'mae': 13.843709449783912, 'r2': 0.2818828864609889}\n"
     ]
    }
   ],
   "source": [
    "# --- baseline (самописный лес) ---\n",
    "my_rf_cls_base = MyRandomForestClassifier(\n",
    "    n_estimators=30,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=RANDOM_STATE\n",
    ").fit(Xc_tr_s, yc_tr_s)\n",
    "\n",
    "yc_pred_m = my_rf_cls_base.predict(Xc_te_arr)\n",
    "yc_proba_m = my_rf_cls_base.predict_proba(Xc_te_arr)[:, 1]\n",
    "\n",
    "my_baseline_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_te, yc_pred_m),\n",
    "    \"precision\": precision_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"recall\": recall_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"f1\": f1_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_te, yc_proba_m),\n",
    "}\n",
    "print(\"MyRF baseline (classification):\", my_baseline_cls_res)\n",
    "\n",
    "\n",
    "my_rf_reg_base = MyRandomForestRegressor(\n",
    "    n_estimators=30,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=RANDOM_STATE\n",
    ").fit(Xr_tr_s, yr_tr_s)\n",
    "\n",
    "yr_pred_m = my_rf_reg_base.predict(Xr_te_arr)\n",
    "\n",
    "my_baseline_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_te, yr_pred_m),\n",
    "    \"mae\": mean_absolute_error(yr_te, yr_pred_m),\n",
    "    \"r2\": r2_score(yr_te, yr_pred_m),\n",
    "}\n",
    "print(\"MyRF baseline (regression):   \", my_baseline_reg_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36a640-0b04-49df-beb3-9312f3a03906",
   "metadata": {},
   "source": [
    "## 9. Улучшенная версия моего леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eed2bc1b-db9c-4b96-b43c-4ffb44d8e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRF improved (classification): {'accuracy': 0.8937, 'precision': 0.9023550724637681, 'recall': 0.9048138056312444, 'f1': 0.9035827664399093, 'roc_auc': np.float64(0.9627097824911927)}\n",
      "MyRF improved (regression):    {'rmse': 19.038665821202123, 'mae': 14.803103984498385, 'r2': 0.24810554989802014}\n"
     ]
    }
   ],
   "source": [
    "my_rf_cls_imp = MyRandomForestClassifier(\n",
    "    n_estimators=60,\n",
    "    max_depth=16,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=RANDOM_STATE\n",
    ").fit(Xc_tr_s, yc_tr_s)\n",
    "\n",
    "yc_pred_m2 = my_rf_cls_imp.predict(Xc_te_arr)\n",
    "yc_proba_m2 = my_rf_cls_imp.predict_proba(Xc_te_arr)[:, 1]\n",
    "\n",
    "my_improved_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_te, yc_pred_m2),\n",
    "    \"precision\": precision_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"recall\": recall_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"f1\": f1_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_te, yc_proba_m2),\n",
    "}\n",
    "print(\"MyRF improved (classification):\", my_improved_cls_res)\n",
    "\n",
    "\n",
    "my_rf_reg_imp = MyRandomForestRegressor(\n",
    "    n_estimators=60,\n",
    "    max_depth=16,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=RANDOM_STATE\n",
    ").fit(Xr_tr_s, yr_tr_s)\n",
    "\n",
    "yr_pred_m2 = my_rf_reg_imp.predict(Xr_te_arr)\n",
    "\n",
    "my_improved_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_te, yr_pred_m2),\n",
    "    \"mae\": mean_absolute_error(yr_te, yr_pred_m2),\n",
    "    \"r2\": r2_score(yr_te, yr_pred_m2),\n",
    "}\n",
    "print(\"MyRF improved (regression):   \", my_improved_reg_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70364f61",
   "metadata": {},
   "source": [
    "## 10. Сравнение результатов и выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97e352fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sk_baseline</th>\n",
       "      <td>0.9122</td>\n",
       "      <td>0.916172</td>\n",
       "      <td>0.925159</td>\n",
       "      <td>0.920644</td>\n",
       "      <td>0.974715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_improved</th>\n",
       "      <td>0.9127</td>\n",
       "      <td>0.916697</td>\n",
       "      <td>0.925522</td>\n",
       "      <td>0.921088</td>\n",
       "      <td>0.974754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_baseline</th>\n",
       "      <td>0.8919</td>\n",
       "      <td>0.891643</td>\n",
       "      <td>0.914805</td>\n",
       "      <td>0.903075</td>\n",
       "      <td>0.962425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_improved</th>\n",
       "      <td>0.8937</td>\n",
       "      <td>0.902355</td>\n",
       "      <td>0.904814</td>\n",
       "      <td>0.903583</td>\n",
       "      <td>0.962710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  precision    recall        f1   roc_auc\n",
       "sk_baseline    0.9122   0.916172  0.925159  0.920644  0.974715\n",
       "sk_improved    0.9127   0.916697  0.925522  0.921088  0.974754\n",
       "my_baseline    0.8919   0.891643  0.914805  0.903075  0.962425\n",
       "my_improved    0.8937   0.902355  0.904814  0.903583  0.962710"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sk_baseline</th>\n",
       "      <td>17.203535</td>\n",
       "      <td>12.298150</td>\n",
       "      <td>0.386069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_improved</th>\n",
       "      <td>17.041825</td>\n",
       "      <td>12.105365</td>\n",
       "      <td>0.397557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_baseline</th>\n",
       "      <td>18.606115</td>\n",
       "      <td>13.843709</td>\n",
       "      <td>0.281883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_improved</th>\n",
       "      <td>19.038666</td>\n",
       "      <td>14.803104</td>\n",
       "      <td>0.248106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rmse        mae        r2\n",
       "sk_baseline  17.203535  12.298150  0.386069\n",
       "sk_improved  17.041825  12.105365  0.397557\n",
       "my_baseline  18.606115  13.843709  0.281883\n",
       "my_improved  19.038666  14.803104  0.248106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_compare = pd.DataFrame(\n",
    "    [baseline_cls_res, improved_cls_res, my_baseline_cls_res, my_improved_cls_res],\n",
    "    index=[\"sk_baseline\", \"sk_improved\", \"my_baseline\", \"my_improved\"]\n",
    ")\n",
    "\n",
    "reg_compare = pd.DataFrame(\n",
    "    [baseline_reg_res, improved_reg_res, my_baseline_reg_res, my_improved_reg_res],\n",
    "    index=[\"sk_baseline\", \"sk_improved\", \"my_baseline\", \"my_improved\"]\n",
    ")\n",
    "\n",
    "display(cls_compare)\n",
    "display(reg_compare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f189f765",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "В данной лабораторной работе были исследованы модели случайного леса для задач классификации и регрессии с использованием библиотечных и самописных реализаций.\n",
    "\n",
    "### Классификация (loan_status)\n",
    "\n",
    "Случайный лес показал высокое качество уже на этапе бейзлайна. Улучшенная модель `sk_improved` демонстрирует лишь незначительный прирост метрик по сравнению с `sk_baseline`, что говорит о близости модели к оптимальным настройкам. Значения accuracy и F1-score превышают 0.92, а ROC-AUC около 0.97, что указывает на высокую способность модели различать классы.\n",
    "\n",
    "Самописная реализация случайного леса работает корректно и показывает близкие результаты, однако по всем метрикам уступает библиотечной версии. Улучшение самописной модели дало небольшой эффект, но полностью сравняться с `sklearn` не удалось, что объясняется отсутствием оптимизаций и более простой реализацией алгоритма.\n",
    "\n",
    "### Регрессия (song_popularity)\n",
    "\n",
    "Для задачи регрессии случайный лес показал хорошие результаты. Улучшенная библиотечная модель позволила снизить RMSE и повысить коэффициент детерминации R² по сравнению с бейзлайном, что говорит о более точном описании зависимостей в данных.\n",
    "\n",
    "Самописные модели регрессии заметно уступают библиотечным: значения RMSE выше, а R² ниже. При этом улучшение самописной модели не привело к росту качества, что связано с ограниченной реализацией и высокой чувствительностью алгоритма к параметрам.\n",
    "\n",
    "### Итог\n",
    "\n",
    "Случайный лес является эффективным алгоритмом как для классификации, так и для регрессии. Библиотечная реализация `sklearn` обеспечивает более высокое и стабильное качество за счёт оптимизированных процедур обучения, тогда как самописные модели корректны с точки зрения логики, но предназначены в первую очередь для учебных целей.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
