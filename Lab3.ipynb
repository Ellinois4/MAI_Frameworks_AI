{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08de3eb",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3  \n",
    "## Проведение исследований с решающим деревом (Decision Tree)\n",
    "\n",
    "В лабораторной работе исследуются модели **решающего дерева**:\n",
    "- **DecisionTreeClassifier** для задачи классификации (loan_status);\n",
    "- **DecisionTreeRegressor** для задачи регрессии (song_popularity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632badd5",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a19b5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    root_mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a51f43",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных и формирование выборок\n",
    "\n",
    "- отделяется целевая переменная;\n",
    "- исключаются служебные идентификаторы (`customer_id`, `song_name`);\n",
    "- определяется список категориальных и числовых признаков;\n",
    "- данные делятся на train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "317d1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: (40000, 18) (10000, 18) | target mean: 0.55045\n",
      "Regression: (15068, 13) (3767, 13) | target mean: 53.06099017786037\n"
     ]
    }
   ],
   "source": [
    "# Пути к данным\n",
    "LOAN_PATH = os.path.join(\"data\", \"Loan_approval_data_2025.csv\")\n",
    "SONG_PATH = os.path.join(\"data\", \"song_data.csv\")\n",
    "\n",
    "# Загрузка датасетов\n",
    "loan_df = pd.read_csv(LOAN_PATH)\n",
    "song_df = pd.read_csv(SONG_PATH)\n",
    "\n",
    "# --- Classification ---\n",
    "y_cls = loan_df[\"loan_status\"].astype(int)\n",
    "X_cls = loan_df.drop(columns=[\"loan_status\"])\n",
    "\n",
    "if \"customer_id\" in X_cls.columns:\n",
    "    X_cls = X_cls.drop(columns=[\"customer_id\"])\n",
    "\n",
    "cat_cols_cls = X_cls.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_cls = X_cls.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=RANDOM_STATE, stratify=y_cls\n",
    ")\n",
    "\n",
    "# --- Regression ---\n",
    "y_reg = song_df[\"song_popularity\"].astype(float)\n",
    "X_reg = song_df.drop(columns=[\"song_popularity\"])\n",
    "\n",
    "if \"song_name\" in X_reg.columns:\n",
    "    X_reg = X_reg.drop(columns=[\"song_name\"])\n",
    "\n",
    "cat_cols_reg = X_reg.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_reg = X_reg.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Classification:\", Xc_train.shape, Xc_test.shape, \"| target mean:\", yc_train.mean())\n",
    "print(\"Regression:\", Xr_train.shape, Xr_test.shape, \"| target mean:\", yr_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d57b648",
   "metadata": {},
   "source": [
    "## 2. Бейзлайн и оценка качества (Decision Tree)\n",
    "\n",
    "Для деревьев масштабирование признаков не является обязательным (модель основана на разбиениях),\n",
    "но для корректной работы с категориальными переменными выполняется One-Hot Encoding.\n",
    "\n",
    "Бейзлайн:\n",
    "- DecisionTreeClassifier с параметрами по умолчанию (фиксируется random_state);\n",
    "- DecisionTreeRegressor с параметрами по умолчанию (фиксируется random_state).\n",
    "\n",
    "Затем рассчитываются метрики:\n",
    "- **классификация**: Accuracy, Precision, Recall, F1, ROC-AUC;\n",
    "- **регрессия**: RMSE, MAE, R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "092facb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classification: {'accuracy': 0.8716, 'precision': 0.8808879263670818, 'recall': 0.8866485013623978, 'f1': 0.8837588267246062, 'roc_auc': np.float64(0.8699093452306984)}\n",
      "Baseline regression: {'rmse': 23.825798540585282, 'mae': 15.846939904180413, 'r2': -0.17754845760785476}\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: impute + one-hot for categorical; impute for numeric\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "prep_cls = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols_cls),\n",
    "    (\"cat\", cat_pipe, cat_cols_cls)\n",
    "])\n",
    "\n",
    "prep_reg = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols_reg),\n",
    "    (\"cat\", cat_pipe, cat_cols_reg)\n",
    "])\n",
    "\n",
    "baseline_cls = Pipeline([\n",
    "    (\"prep\", prep_cls),\n",
    "    (\"model\", DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "baseline_reg = Pipeline([\n",
    "    (\"prep\", prep_reg),\n",
    "    (\"model\", DecisionTreeRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "baseline_cls.fit(Xc_train, yc_train)\n",
    "baseline_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "yc_pred = baseline_cls.predict(Xc_test)\n",
    "yc_proba = baseline_cls.predict_proba(Xc_test)[:, 1]\n",
    "\n",
    "yr_pred = baseline_reg.predict(Xr_test)\n",
    "\n",
    "baseline_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_test, yc_pred),\n",
    "    \"precision\": precision_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"recall\": recall_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"f1\": f1_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_test, yc_proba)\n",
    "}\n",
    "\n",
    "baseline_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_test, yr_pred),\n",
    "    \"mae\": mean_absolute_error(yr_test, yr_pred),\n",
    "    \"r2\": r2_score(yr_test, yr_pred)\n",
    "}\n",
    "\n",
    "print(\"Baseline classification:\", baseline_cls_res)\n",
    "print(\"Baseline regression:\", baseline_reg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4dc86",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна: гипотезы и проверка\n",
    "\n",
    "### Гипотезы для классификации\n",
    "1) Ограничение глубины дерева и минимального числа объектов в листе снижает переобучение → улучшает ROC-AUC/F1.  \n",
    "2) Использование отсечения по стоимости сложности (`ccp_alpha`) может дополнительно уменьшить переобучение.\n",
    "\n",
    "### Гипотезы для регрессии\n",
    "1) Аналогично, ограничение глубины/минимального размера листа уменьшает дисперсию → снижает RMSE.  \n",
    "2) Параметр `max_features` может улучшать обобщение за счёт уменьшения корреляции разбиений.\n",
    "\n",
    "Ниже проводится GridSearchCV:\n",
    "- для классификации оптимизация по ROC-AUC;\n",
    "- для регрессии оптимизация по neg-RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca17362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cls params: {'model__ccp_alpha': 0.0, 'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10, 'model__min_samples_split': 2}\n",
      "Best CV ROC-AUC: 0.9513020143940724\n",
      "Improved classification: {'accuracy': 0.888, 'precision': 0.8896392393815532, 'recall': 0.9093551316984559, 'f1': 0.8993891484010061, 'roc_auc': np.float64(0.9532776654654128)}\n",
      "Best reg params: {'model__ccp_alpha': 0.0, 'model__criterion': 'squared_error', 'model__max_depth': 8, 'model__max_features': None, 'model__min_samples_leaf': 5, 'model__min_samples_split': 20}\n",
      "Best CV -RMSE: -20.994976298583495\n",
      "Improved regression: {'rmse': 20.946312882363923, 'mae': 16.475162067214562, 'r2': 0.08987922013665017}\n"
     ]
    }
   ],
   "source": [
    "# --- Tuning: Classification ---\n",
    "param_grid_cls = {\n",
    "    \"model__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"model__max_depth\": [None, 5, 10],\n",
    "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"model__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"model__ccp_alpha\": [0.0, 1e-3]\n",
    "}\n",
    "\n",
    "\n",
    "tuned_cls = Pipeline([\n",
    "    (\"prep\", prep_cls),\n",
    "    (\"model\", DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "cv_cls = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs_cls = GridSearchCV(\n",
    "    tuned_cls,\n",
    "    param_grid=param_grid_cls,\n",
    "    cv=cv_cls,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_cls.fit(Xc_train, yc_train)\n",
    "\n",
    "best_cls = gs_cls.best_estimator_\n",
    "print(\"Best cls params:\", gs_cls.best_params_)\n",
    "print(\"Best CV ROC-AUC:\", gs_cls.best_score_)\n",
    "\n",
    "yc_pred2 = best_cls.predict(Xc_test)\n",
    "yc_proba2 = best_cls.predict_proba(Xc_test)[:, 1]\n",
    "\n",
    "improved_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_test, yc_pred2),\n",
    "    \"precision\": precision_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"recall\": recall_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"f1\": f1_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_test, yc_proba2)\n",
    "}\n",
    "print(\"Improved classification:\", improved_cls_res)\n",
    "\n",
    "# --- Tuning: Regression ---\n",
    "param_grid_reg = {\n",
    "    \"model__criterion\": [\"squared_error\", \"friedman_mse\"],\n",
    "    \"model__max_depth\": [3, 5, 8, 12],\n",
    "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"model__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"model__max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"model__ccp_alpha\": [0.0, 1e-3]\n",
    "}\n",
    "\n",
    "tuned_reg = Pipeline([\n",
    "    (\"prep\", prep_reg),\n",
    "    (\"model\", DecisionTreeRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "cv_reg = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs_reg = GridSearchCV(\n",
    "    tuned_reg,\n",
    "    param_grid=param_grid_reg,\n",
    "    cv=cv_reg,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "best_reg = gs_reg.best_estimator_\n",
    "print(\"Best reg params:\", gs_reg.best_params_)\n",
    "print(\"Best CV -RMSE:\", gs_reg.best_score_)\n",
    "\n",
    "yr_pred2 = best_reg.predict(Xr_test)\n",
    "\n",
    "improved_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_test, yr_pred2),\n",
    "    \"mae\": mean_absolute_error(yr_test, yr_pred2),\n",
    "    \"r2\": r2_score(yr_test, yr_pred2)\n",
    "}\n",
    "print(\"Improved regression:\", improved_reg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06bade-e9c1-45cf-9159-fc6e3812b2e1",
   "metadata": {},
   "source": [
    "## 4. Имплементация решающего дерева\n",
    "\n",
    "Далее реализуются версии решающего дерева:\n",
    "- для классификации: критерий Джини (Gini);\n",
    "- для регрессии: MSE.\n",
    "\n",
    "Из-за вычислительной сложности самописное дерево обучается на подвыборке. Для сравнения используются те же метрики, что и для sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea99c56-dcea-4815-aae4-f99e10eebd02",
   "metadata": {},
   "source": [
    "## 5. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b06b3b6b-23b6-4620-a280-a29c3a59b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS: (40000, 27) (10000, 27)\n",
      "REG: (15068, 13) (3767, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# чтобы результаты были воспроизводимыми\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "def to_dense(X):\n",
    "    return X.toarray() if sparse.issparse(X) else np.asarray(X)\n",
    "\n",
    "def sample_indices(n, max_n=6000, random_state=42):\n",
    "    # подвыборка для ускорения самописного дерева\n",
    "    if n <= max_n:\n",
    "        return np.arange(n)\n",
    "    rng_local = np.random.default_rng(random_state)\n",
    "    return rng_local.choice(n, size=max_n, replace=False)\n",
    "\n",
    "# --- Классификация: подготовка матриц ---\n",
    "Xc_tr = prep_cls.fit_transform(Xc_train)\n",
    "Xc_te = prep_cls.transform(Xc_test)\n",
    "\n",
    "yc_tr = np.asarray(yc_train).astype(int)\n",
    "yc_te = np.asarray(yc_test).astype(int)\n",
    "\n",
    "Xc_tr_arr = to_dense(Xc_tr)\n",
    "Xc_te_arr = to_dense(Xc_te)\n",
    "\n",
    "# --- Регрессия: подготовка матриц ---\n",
    "Xr_tr = prep_reg.fit_transform(Xr_train)\n",
    "Xr_te = prep_reg.transform(Xr_test)\n",
    "\n",
    "yr_tr = np.asarray(yr_train).astype(float)\n",
    "yr_te = np.asarray(yr_test).astype(float)\n",
    "\n",
    "Xr_tr_arr = to_dense(Xr_tr)\n",
    "Xr_te_arr = to_dense(Xr_te)\n",
    "\n",
    "print(\"CLS:\", Xc_tr_arr.shape, Xc_te_arr.shape)\n",
    "print(\"REG:\", Xr_tr_arr.shape, Xr_te_arr.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18341dd4-7fb5-49f9-8a17-3c077d06d7fb",
   "metadata": {},
   "source": [
    "## 6. Имплементация общей базы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3e165c3-7447-4d60-a9ab-869ec5f1a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Node:\n",
    "    __slots__ = (\"feature\", \"threshold\", \"left\", \"right\", \"value\")\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value  # для листа: класс (int) или среднее (float)\n",
    "\n",
    "class MyDecisionTreeBase:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=42):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = int(min_samples_split)\n",
    "        self.min_samples_leaf = int(min_samples_leaf)\n",
    "        self.max_features = max_features\n",
    "        self.random_state = int(random_state)\n",
    "        self.root_ = None\n",
    "        self.n_features_ = None\n",
    "        self.rng_ = np.random.default_rng(self.random_state)\n",
    "\n",
    "    def _feature_subset(self, n_features):\n",
    "        if self.max_features is None:\n",
    "            return np.arange(n_features)\n",
    "        if self.max_features == \"sqrt\":\n",
    "            k = max(1, int(np.sqrt(n_features)))\n",
    "        elif self.max_features == \"log2\":\n",
    "            k = max(1, int(np.log2(n_features)))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            k = max(1, min(n_features, self.max_features))\n",
    "        else:\n",
    "            k = n_features\n",
    "        return self.rng_.choice(n_features, size=k, replace=False)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.root_ = self._build(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build(self, X, y, depth):\n",
    "        # стоп-условия\n",
    "        n = X.shape[0]\n",
    "        if n < self.min_samples_split:\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "\n",
    "        # если узел \"чистый\" (в классификации) или нет смысла делить (в регрессии)\n",
    "        if self._is_pure(y):\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "\n",
    "        feat_ids = self._feature_subset(self.n_features_)\n",
    "        best = self._best_split(X, y, feat_ids)\n",
    "\n",
    "        if best is None:\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "\n",
    "        f, thr = best\n",
    "        mask = X[:, f] <= thr\n",
    "        n_left = int(mask.sum())\n",
    "        n_right = n - n_left\n",
    "\n",
    "        if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
    "            return _Node(value=self._leaf_value(y))\n",
    "\n",
    "        left = self._build(X[mask], y[mask], depth + 1)\n",
    "        right = self._build(X[~mask], y[~mask], depth + 1)\n",
    "        return _Node(feature=f, threshold=thr, left=left, right=right, value=None)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        out = np.empty(X.shape[0], dtype=float)\n",
    "        for i in range(X.shape[0]):\n",
    "            out[i] = self._predict_one(X[i], self.root_)\n",
    "        return out\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        while node.value is None:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n",
    "\n",
    "    # --- методы, которые переопределяются в наследниках ---\n",
    "    def _leaf_value(self, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _is_pure(self, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _best_split(self, X, y, feat_ids):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a4761-3145-4cdf-bb96-40f81267a7ca",
   "metadata": {},
   "source": [
    "## 7. дерево классификации (Gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f84a3c08-24bc-47fc-a7d1-8c1763f68170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDecisionTreeClassifier defined.\n"
     ]
    }
   ],
   "source": [
    "class MyDecisionTreeClassifier(MyDecisionTreeBase):\n",
    "    def _leaf_value(self, y):\n",
    "        # мажоритарный класс\n",
    "        vals, cnts = np.unique(y, return_counts=True)\n",
    "        return int(vals[np.argmax(cnts)])\n",
    "\n",
    "    def _is_pure(self, y):\n",
    "        return np.unique(y).size == 1\n",
    "\n",
    "    def _gini(self, y):\n",
    "        _, cnts = np.unique(y, return_counts=True)\n",
    "        p = cnts / cnts.sum()\n",
    "        return 1.0 - np.sum(p * p)\n",
    "\n",
    "    def _best_split(self, X, y, feat_ids):\n",
    "        n = X.shape[0]\n",
    "        base = self._gini(y)\n",
    "        best_gain = 0.0\n",
    "        best = None\n",
    "\n",
    "        for f in feat_ids:\n",
    "            x = X[:, f]\n",
    "            # кандидаты порогов (берём уникальные значения, но ограничим)\n",
    "            uniq = np.unique(x)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "            # чтобы не было слишком долго: максимум 50 порогов\n",
    "            if uniq.size > 50:\n",
    "                qs = np.linspace(0.05, 0.95, 50)\n",
    "                thr_list = np.quantile(uniq, qs)\n",
    "            else:\n",
    "                thr_list = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "\n",
    "            for thr in thr_list:\n",
    "                mask = x <= thr\n",
    "                if mask.sum() < self.min_samples_leaf or (~mask).sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                g_left = self._gini(y[mask])\n",
    "                g_right = self._gini(y[~mask])\n",
    "                w = mask.mean()\n",
    "                impur = w * g_left + (1 - w) * g_right\n",
    "                gain = base - impur\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best = (f, float(thr))\n",
    "\n",
    "        return best\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # простая вероятность по голосованию листа (0/1)\n",
    "        pred = self.predict(X).astype(int)\n",
    "        proba1 = pred.astype(float)\n",
    "        proba0 = 1.0 - proba1\n",
    "        return np.vstack([proba0, proba1]).T\n",
    "\n",
    "print(\"MyDecisionTreeClassifier defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4bf2b-9c2d-4d4b-9f52-6a626cf099f7",
   "metadata": {},
   "source": [
    "## 8. дерево регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "894fa9dc-5f62-410d-a05c-54aa8cfb67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDecisionTreeRegressor defined.\n"
     ]
    }
   ],
   "source": [
    "class MyDecisionTreeRegressor(MyDecisionTreeBase):\n",
    "    def _leaf_value(self, y):\n",
    "        return float(np.mean(y))\n",
    "\n",
    "    def _is_pure(self, y):\n",
    "        # для регрессии считаем \"чистым\", если дисперсия почти нулевая\n",
    "        return float(np.var(y)) < 1e-12\n",
    "\n",
    "    def _mse(self, y):\n",
    "        if y.size == 0:\n",
    "            return 0.0\n",
    "        m = float(np.mean(y))\n",
    "        return float(np.mean((y - m) ** 2))\n",
    "\n",
    "    def _best_split(self, X, y, feat_ids):\n",
    "        base = self._mse(y)\n",
    "        best_gain = 0.0\n",
    "        best = None\n",
    "\n",
    "        for f in feat_ids:\n",
    "            x = X[:, f]\n",
    "            uniq = np.unique(x)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "            # ограничим пороги\n",
    "            if uniq.size > 50:\n",
    "                qs = np.linspace(0.05, 0.95, 50)\n",
    "                thr_list = np.quantile(uniq, qs)\n",
    "            else:\n",
    "                thr_list = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "\n",
    "            for thr in thr_list:\n",
    "                mask = x <= thr\n",
    "                if mask.sum() < self.min_samples_leaf or (~mask).sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                mse_left = self._mse(y[mask])\n",
    "                mse_right = self._mse(y[~mask])\n",
    "                w = mask.mean()\n",
    "                mse_split = w * mse_left + (1 - w) * mse_right\n",
    "                gain = base - mse_split\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best = (f, float(thr))\n",
    "\n",
    "        return best\n",
    "\n",
    "print(\"MyDecisionTreeRegressor defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e6317-1fe0-4653-bb8d-db3b2acea343",
   "metadata": {},
   "source": [
    "## 9. Обучение/оценка своих моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "391a1547-2262-4546-b04b-542d16a22dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTree baseline (classification): {'accuracy': 0.833, 'precision': 0.8469332368373439, 'recall': 0.8503178928247048, 'f1': 0.8486221899927484, 'roc_auc': np.float64(0.8310543857894381)}\n",
      "MyTree baseline (regression):    {'rmse': 26.780227019996392, 'mae': 19.21166268471817, 'r2': -0.4876897333309129}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- подвыборка для ускорения ---\n",
    "idx_c = sample_indices(Xc_tr_arr.shape[0], max_n=6000, random_state=RANDOM_STATE)\n",
    "idx_r = sample_indices(Xr_tr_arr.shape[0], max_n=6000, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- MyTree baseline params (похоже на бейзлайн sklearn) ---\n",
    "my_tree_cls_base = MyDecisionTreeClassifier(\n",
    "    max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=RANDOM_STATE\n",
    ").fit(Xc_tr_arr[idx_c], yc_tr[idx_c])\n",
    "\n",
    "yc_pred_m = my_tree_cls_base.predict(Xc_te_arr).astype(int)\n",
    "yc_proba_m = my_tree_cls_base.predict_proba(Xc_te_arr)[:, 1]\n",
    "\n",
    "my_baseline_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_te, yc_pred_m),\n",
    "    \"precision\": precision_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"recall\": recall_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"f1\": f1_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_te, yc_proba_m),\n",
    "}\n",
    "print(\"MyTree baseline (classification):\", my_baseline_cls_res)\n",
    "\n",
    "my_tree_reg_base = MyDecisionTreeRegressor(\n",
    "    max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=RANDOM_STATE\n",
    ").fit(Xr_tr_arr[idx_r], yr_tr[idx_r])\n",
    "\n",
    "yr_pred_m = my_tree_reg_base.predict(Xr_te_arr)\n",
    "\n",
    "my_baseline_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_te, yr_pred_m),\n",
    "    \"mae\": mean_absolute_error(yr_te, yr_pred_m),\n",
    "    \"r2\": r2_score(yr_te, yr_pred_m),\n",
    "}\n",
    "print(\"MyTree baseline (regression):   \", my_baseline_reg_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab936a6f-72d7-4061-8cb0-a64de75e4c4d",
   "metadata": {},
   "source": [
    "## 10. улучшенный самописный вариант \n",
    "\n",
    "Здесь логика простая: берём техники улучшенного бейзлайна как ограничение глубины + увеличение min_samples_leaf + выбор max_features, чтобы уменьшить переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63bed68d-0a5a-4c86-bdb0-f07cf1be3416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTree improved (classification): {'accuracy': 0.8135, 'precision': 0.8408239700374532, 'recall': 0.815622161671208, 'f1': 0.8280313508529276, 'roc_auc': np.float64(0.8132615813917775)}\n",
      "MyTree improved (regression):    {'rmse': 21.940023724020975, 'mae': 17.374964420720254, 'r2': 0.0014770685134980477}\n"
     ]
    }
   ],
   "source": [
    "# Улучшенные параметры (идея: меньше переобучения)\n",
    "my_tree_cls_imp = MyDecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=RANDOM_STATE\n",
    ").fit(Xc_tr_arr[idx_c], yc_tr[idx_c])\n",
    "\n",
    "yc_pred_m2 = my_tree_cls_imp.predict(Xc_te_arr).astype(int)\n",
    "yc_proba_m2 = my_tree_cls_imp.predict_proba(Xc_te_arr)[:, 1]\n",
    "\n",
    "my_improved_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_te, yc_pred_m2),\n",
    "    \"precision\": precision_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"recall\": recall_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"f1\": f1_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_te, yc_proba_m2),\n",
    "}\n",
    "print(\"MyTree improved (classification):\", my_improved_cls_res)\n",
    "\n",
    "my_tree_reg_imp = MyDecisionTreeRegressor(\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=RANDOM_STATE\n",
    ").fit(Xr_tr_arr[idx_r], yr_tr[idx_r])\n",
    "\n",
    "yr_pred_m2 = my_tree_reg_imp.predict(Xr_te_arr)\n",
    "\n",
    "my_improved_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_te, yr_pred_m2),\n",
    "    \"mae\": mean_absolute_error(yr_te, yr_pred_m2),\n",
    "    \"r2\": r2_score(yr_te, yr_pred_m2),\n",
    "}\n",
    "print(\"MyTree improved (regression):   \", my_improved_reg_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c07662",
   "metadata": {},
   "source": [
    "## 11. Сравнение результатов и выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "651a663e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sk_baseline</th>\n",
       "      <td>0.8716</td>\n",
       "      <td>0.880888</td>\n",
       "      <td>0.886649</td>\n",
       "      <td>0.883759</td>\n",
       "      <td>0.869909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_improved</th>\n",
       "      <td>0.8880</td>\n",
       "      <td>0.889639</td>\n",
       "      <td>0.909355</td>\n",
       "      <td>0.899389</td>\n",
       "      <td>0.953278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_baseline</th>\n",
       "      <td>0.8330</td>\n",
       "      <td>0.846933</td>\n",
       "      <td>0.850318</td>\n",
       "      <td>0.848622</td>\n",
       "      <td>0.831054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_improved</th>\n",
       "      <td>0.8135</td>\n",
       "      <td>0.840824</td>\n",
       "      <td>0.815622</td>\n",
       "      <td>0.828031</td>\n",
       "      <td>0.813262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  precision    recall        f1   roc_auc\n",
       "sk_baseline    0.8716   0.880888  0.886649  0.883759  0.869909\n",
       "sk_improved    0.8880   0.889639  0.909355  0.899389  0.953278\n",
       "my_baseline    0.8330   0.846933  0.850318  0.848622  0.831054\n",
       "my_improved    0.8135   0.840824  0.815622  0.828031  0.813262"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sk_baseline</th>\n",
       "      <td>23.825799</td>\n",
       "      <td>15.846940</td>\n",
       "      <td>-0.177548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_improved</th>\n",
       "      <td>20.946313</td>\n",
       "      <td>16.475162</td>\n",
       "      <td>0.089879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_baseline</th>\n",
       "      <td>26.780227</td>\n",
       "      <td>19.211663</td>\n",
       "      <td>-0.487690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_improved</th>\n",
       "      <td>21.940024</td>\n",
       "      <td>17.374964</td>\n",
       "      <td>0.001477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rmse        mae        r2\n",
       "sk_baseline  23.825799  15.846940 -0.177548\n",
       "sk_improved  20.946313  16.475162  0.089879\n",
       "my_baseline  26.780227  19.211663 -0.487690\n",
       "my_improved  21.940024  17.374964  0.001477"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cls_compare = pd.DataFrame(\n",
    "    [baseline_cls_res, improved_cls_res, my_baseline_cls_res, my_improved_cls_res],\n",
    "    index=[\"sk_baseline\", \"sk_improved\", \"my_baseline\", \"my_improved\"]\n",
    ")\n",
    "\n",
    "reg_compare = pd.DataFrame(\n",
    "    [baseline_reg_res, improved_reg_res, my_baseline_reg_res, my_improved_reg_res],\n",
    "    index=[\"sk_baseline\", \"sk_improved\", \"my_baseline\", \"my_improved\"]\n",
    ")\n",
    "\n",
    "display(cls_compare)\n",
    "display(reg_compare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eefe4f2",
   "metadata": {},
   "source": [
    "## Выводы по лабораторной работе №3 (Решающее дерево)\n",
    "\n",
    "В данной лабораторной работе были исследованы модели решающего дерева для задач классификации и регрессии, включая библиотечные реализации `sklearn` и самописные модели.\n",
    "\n",
    "### Классификация (loan_status)\n",
    "\n",
    "Подбор гиперпараметров решающего дерева позволил улучшить качество классификации.  \n",
    "Улучшенная модель `sk_improved` показала рост всех основных метрик по сравнению с бейзлайном:\n",
    "- accuracy увеличилась с **0.872 → 0.888**;\n",
    "- F1-score вырос с **0.884 → 0.899**;\n",
    "- ROC-AUC повысился с **0.870 → 0.953**, что говорит о лучшей способности модели различать классы.\n",
    "\n",
    "Самописная реализация решающего дерева работает корректно, однако заметно уступает библиотечной версии:\n",
    "- значения accuracy и ROC-AUC ниже, чем у моделей `sklearn`;\n",
    "- улучшение самописной модели дало незначительный эффект.\n",
    "\n",
    "Это объясняется отсутствием оптимизаций и упрощённой логикой построения дерева.\n",
    "\n",
    "### Регрессия (song_popularity)\n",
    "\n",
    "Для задачи регрессии решающие деревья показали более слабые результаты.  \n",
    "Даже после подбора гиперпараметров модель `sk_improved` демонстрирует:\n",
    "- снижение RMSE (с **23.83 → 20.95**);\n",
    "- рост R² до **0.09**, однако объясняемая дисперсия остаётся низкой.\n",
    "\n",
    "Самописная регрессионная модель показала худшие результаты:\n",
    "- отрицательные значения R² указывают на слабую способность модели описывать зависимость;\n",
    "- высокая чувствительность дерева к шуму приводит к переобучению.\n",
    "\n",
    "### Итог\n",
    "\n",
    "Решающие деревья хорошо подходят для задач классификации и выигрывают от настройки гиперпараметров.  \n",
    "В задаче регрессии их эффективность ограничена, особенно без ансамблевых методов.  \n",
    "Библиотечные реализации `sklearn` значительно превосходят самописные модели за счёт оптимизаций и устойчивых алгоритмов построения деревьев.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
