{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc822ea",
   "metadata": {},
   "source": [
    "# Лабораторная работа №5  \n",
    "## Проведение исследований с градиентным бустингом (Gradient Boosting)\n",
    "\n",
    "В лабораторной работе исследуются модели градиентного бустинга:\n",
    "- **GradientBoostingClassifier** для задачи классификации (loan_status);\n",
    "- **GradientBoostingRegressor** для задачи регрессии (song_popularity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d22fba",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00eff3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    root_mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9770e9fe",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных и формирование выборок\n",
    "\n",
    "- отделяется целевая переменная;\n",
    "- удаляются идентификаторы (`customer_id`, `song_name`);\n",
    "- выделяются категориальные и числовые признаки;\n",
    "- выполняется разбиение train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4c0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: (40000, 18) (10000, 18)\n",
      "Regression: (15068, 13) (3767, 13)\n"
     ]
    }
   ],
   "source": [
    "# Пути к данным\n",
    "LOAN_PATH = os.path.join(\"data\", \"Loan_approval_data_2025.csv\")\n",
    "SONG_PATH = os.path.join(\"data\", \"song_data.csv\")\n",
    "\n",
    "# Загрузка датасетов\n",
    "loan_df = pd.read_csv(LOAN_PATH)\n",
    "song_df = pd.read_csv(SONG_PATH)\n",
    "\n",
    "# --- Classification ---\n",
    "y_cls = loan_df[\"loan_status\"].astype(int)\n",
    "X_cls = loan_df.drop(columns=[\"loan_status\"])\n",
    "\n",
    "if \"customer_id\" in X_cls.columns:\n",
    "    X_cls = X_cls.drop(columns=[\"customer_id\"])\n",
    "\n",
    "cat_cols_cls = X_cls.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_cls = X_cls.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=RANDOM_STATE, stratify=y_cls\n",
    ")\n",
    "\n",
    "# --- Regression ---\n",
    "y_reg = song_df[\"song_popularity\"].astype(float)\n",
    "X_reg = song_df.drop(columns=[\"song_popularity\"])\n",
    "\n",
    "if \"song_name\" in X_reg.columns:\n",
    "    X_reg = X_reg.drop(columns=[\"song_name\"])\n",
    "\n",
    "cat_cols_reg = X_reg.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_reg = X_reg.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Classification:\", Xc_train.shape, Xc_test.shape)\n",
    "print(\"Regression:\", Xr_train.shape, Xr_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728797d8",
   "metadata": {},
   "source": [
    "## 2. Бейзлайн и оценка качества (Gradient Boosting)\n",
    "\n",
    "Построение бейзлайна:\n",
    "- GradientBoostingClassifier;\n",
    "- GradientBoostingRegressor.\n",
    "\n",
    "Алгоритм работает с числовыми признаками, поэтому категориальные кодируются через one-hot.\n",
    "Масштабирование не обязательно для деревьев, поэтому не используется.\n",
    "\n",
    "Затем рассчитываются метрики:\n",
    "- классификация: Accuracy, Precision, Recall, F1, ROC-AUC;\n",
    "- регрессия: RMSE, MAE, R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847b36de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classification: {'accuracy': 0.9194, 'precision': 0.913135220678741, 'recall': 0.943324250681199, 'f1': 0.9279842744817727, 'roc_auc': np.float64(0.9797343298993028)}\n",
      "Baseline regression: {'rmse': 20.705977881928618, 'mae': 16.41457783620347, 'r2': 0.11064459479883026}\n"
     ]
    }
   ],
   "source": [
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "prep_cls = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols_cls),\n",
    "    (\"cat\", cat_pipe, cat_cols_cls)\n",
    "])\n",
    "\n",
    "prep_reg = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols_reg),\n",
    "    (\"cat\", cat_pipe, cat_cols_reg)\n",
    "])\n",
    "\n",
    "baseline_cls = Pipeline([\n",
    "    (\"prep\", prep_cls),\n",
    "    (\"model\", GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "baseline_reg = Pipeline([\n",
    "    (\"prep\", prep_reg),\n",
    "    (\"model\", GradientBoostingRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "baseline_cls.fit(Xc_train, yc_train)\n",
    "baseline_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "yc_pred = baseline_cls.predict(Xc_test)\n",
    "yc_proba = baseline_cls.predict_proba(Xc_test)[:, 1]\n",
    "\n",
    "yr_pred = baseline_reg.predict(Xr_test)\n",
    "\n",
    "baseline_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_test, yc_pred),\n",
    "    \"precision\": precision_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"recall\": recall_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"f1\": f1_score(yc_test, yc_pred, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_test, yc_proba)\n",
    "}\n",
    "\n",
    "baseline_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_test, yr_pred),\n",
    "    \"mae\": mean_absolute_error(yr_test, yr_pred),\n",
    "    \"r2\": r2_score(yr_test, yr_pred)\n",
    "}\n",
    "\n",
    "print(\"Baseline classification:\", baseline_cls_res)\n",
    "print(\"Baseline regression:\", baseline_reg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b773c",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна: гипотезы и проверка\n",
    "\n",
    "### Гипотезы для классификации\n",
    "1) Подбор `n_estimators`, `learning_rate`, `max_depth` (через `max_depth` базового дерева) улучшит ROC-AUC/F1.  \n",
    "2) Подбор `subsample` (стохастический бустинг) может снизить переобучение.\n",
    "\n",
    "### Гипотезы для регрессии\n",
    "1) Аналогичный подбор параметров уменьшит RMSE.  \n",
    "2) Ограничение глубины деревьев (через `max_depth`) улучшит обобщающую способность.\n",
    "\n",
    "Ниже проводится GridSearchCV:\n",
    "- классификация оптимизируется по ROC-AUC;\n",
    "- регрессия оптимизируется по neg-RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dcb393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best cls params: {'model__learning_rate': np.float64(0.14262878834017695), 'model__max_depth': 3, 'model__n_estimators': 213, 'model__subsample': np.float64(0.8019854157170472)}\n",
      "Best CV ROC-AUC: 0.9843817250544618\n",
      "Improved classification: {'accuracy': 0.9278, 'precision': 0.9262163607200142, 'recall': 0.9440508628519527, 'f1': 0.9350485786254048, 'roc_auc': np.float64(0.9842844860421156)}\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best reg params: {'model__learning_rate': np.float64(0.06925195035576534), 'model__max_depth': 4, 'model__n_estimators': 251, 'model__subsample': np.float64(0.8737942275278175)}\n",
      "Best CV -RMSE: -19.877489284584122\n",
      "Improved regression: {'rmse': 19.972979883216087, 'mae': 15.856033887757498, 'r2': 0.17249698166894978}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Tuning: Classification (RandomizedSearch) ---\n",
    "param_dist_cls = {\n",
    "    \"model__n_estimators\": randint(80, 260),          # было [100,200,400]\n",
    "    \"model__learning_rate\": uniform(0.03, 0.17),      # примерно 0.03..0.20\n",
    "    \"model__max_depth\": randint(2, 5),                # 2..4\n",
    "    \"model__subsample\": uniform(0.75, 0.25),          # 0.75..1.0\n",
    "}\n",
    "\n",
    "tuned_cls = Pipeline([\n",
    "    (\"prep\", prep_cls),\n",
    "    (\"model\", GradientBoostingClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_iter_no_change=10,   # ранняя остановка\n",
    "        validation_fraction=0.1,\n",
    "        tol=1e-4\n",
    "    ))\n",
    "])\n",
    "\n",
    "cv_cls = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "rs_cls = RandomizedSearchCV(\n",
    "    tuned_cls,\n",
    "    param_distributions=param_dist_cls,\n",
    "    n_iter=20,                # 20 вместо полного перебора 54\n",
    "    cv=cv_cls,\n",
    "    scoring=\"roc_auc\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rs_cls.fit(Xc_train, yc_train)\n",
    "\n",
    "best_cls = rs_cls.best_estimator_\n",
    "print(\"Best cls params:\", rs_cls.best_params_)\n",
    "print(\"Best CV ROC-AUC:\", rs_cls.best_score_)\n",
    "\n",
    "yc_pred2 = best_cls.predict(Xc_test)\n",
    "yc_proba2 = best_cls.predict_proba(Xc_test)[:, 1]\n",
    "\n",
    "improved_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_test, yc_pred2),\n",
    "    \"precision\": precision_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"recall\": recall_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"f1\": f1_score(yc_test, yc_pred2, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_test, yc_proba2)\n",
    "}\n",
    "print(\"Improved classification:\", improved_cls_res)\n",
    "\n",
    "\n",
    "# --- Tuning: Regression (RandomizedSearch) ---\n",
    "param_dist_reg = {\n",
    "    \"model__n_estimators\": randint(80, 260),\n",
    "    \"model__learning_rate\": uniform(0.03, 0.17),\n",
    "    \"model__max_depth\": randint(2, 5),\n",
    "    \"model__subsample\": uniform(0.75, 0.25),\n",
    "}\n",
    "\n",
    "tuned_reg = Pipeline([\n",
    "    (\"prep\", prep_reg),\n",
    "    (\"model\", GradientBoostingRegressor(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_iter_no_change=10,   # ранняя остановка\n",
    "        validation_fraction=0.1,\n",
    "        tol=1e-4\n",
    "    ))\n",
    "])\n",
    "\n",
    "cv_reg = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "rs_reg = RandomizedSearchCV(\n",
    "    tuned_reg,\n",
    "    param_distributions=param_dist_reg,\n",
    "    n_iter=20,\n",
    "    cv=cv_reg,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rs_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "best_reg = rs_reg.best_estimator_\n",
    "print(\"Best reg params:\", rs_reg.best_params_)\n",
    "print(\"Best CV -RMSE:\", rs_reg.best_score_)\n",
    "\n",
    "yr_pred2 = best_reg.predict(Xr_test)\n",
    "\n",
    "improved_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_test, yr_pred2),\n",
    "    \"mae\": mean_absolute_error(yr_test, yr_pred2),\n",
    "    \"r2\": r2_score(yr_test, yr_pred2)\n",
    "}\n",
    "print(\"Improved regression:\", improved_reg_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9562ce-8c32-450c-8e63-ace1cf0277c1",
   "metadata": {},
   "source": [
    "## 4. Имплементация градиентного бустинга\n",
    "\n",
    "Реализованы упрощённые версии градиентного бустинга:\n",
    "- для регрессии: оптимизация MSE через обучение на остатках;\n",
    "- для классификации: оптимизация логистической функции потерь через псевдоостатки (y − p).\n",
    "\n",
    "В качестве базовых моделей используются простые деревья глубины 1 (decision stump), чтобы обучение было быстрым и понятным.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b82c4-1ea2-4b4a-beda-15bd4c507b5a",
   "metadata": {},
   "source": [
    "## 5. Утилиты и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982c5467-7d07-4aec-a26b-0eb42d01abd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS: (40000, 27) (10000, 27)\n",
      "REG: (15068, 13) (3767, 13)\n",
      "Subsample CLS: (12000, 27) Subsample REG: (12000, 13)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "def to_dense(X):\n",
    "    return X.toarray() if sparse.issparse(X) else np.asarray(X)\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sample_indices(n, max_n=12000, random_state=42):\n",
    "    if n <= max_n:\n",
    "        return np.arange(n)\n",
    "    r = np.random.default_rng(random_state)\n",
    "    return r.choice(n, size=max_n, replace=False)\n",
    "\n",
    "# Препроцессинг тот же, что и для sklearn\n",
    "Xc_tr = prep_cls.fit_transform(Xc_train)\n",
    "Xc_te = prep_cls.transform(Xc_test)\n",
    "yc_tr = np.asarray(yc_train).astype(int)\n",
    "yc_te = np.asarray(yc_test).astype(int)\n",
    "\n",
    "Xr_tr = prep_reg.fit_transform(Xr_train)\n",
    "Xr_te = prep_reg.transform(Xr_test)\n",
    "yr_tr = np.asarray(yr_train).astype(float)\n",
    "yr_te = np.asarray(yr_test).astype(float)\n",
    "\n",
    "# Для самописного бустинга удобнее dense\n",
    "Xc_tr_arr, Xc_te_arr = to_dense(Xc_tr), to_dense(Xc_te)\n",
    "Xr_tr_arr, Xr_te_arr = to_dense(Xr_tr), to_dense(Xr_te)\n",
    "\n",
    "print(\"CLS:\", Xc_tr_arr.shape, Xc_te_arr.shape)\n",
    "print(\"REG:\", Xr_tr_arr.shape, Xr_te_arr.shape)\n",
    "\n",
    "# Подвыборка для ускорения\n",
    "idx_c = sample_indices(Xc_tr_arr.shape[0], max_n=12000, random_state=RANDOM_STATE)\n",
    "idx_r = sample_indices(Xr_tr_arr.shape[0], max_n=12000, random_state=RANDOM_STATE)\n",
    "\n",
    "Xc_tr_s, yc_tr_s = Xc_tr_arr[idx_c], yc_tr[idx_c]\n",
    "Xr_tr_s, yr_tr_s = Xr_tr_arr[idx_r], yr_tr[idx_r]\n",
    "\n",
    "print(\"Subsample CLS:\", Xc_tr_s.shape, \"Subsample REG:\", Xr_tr_s.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c2d3f-5a3d-48f5-af92-d4b626749bf2",
   "metadata": {},
   "source": [
    "## 6. Моя базовая модель: Decision Stump (регрессия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9e02ca-e25d-494e-a7fa-75c56e79ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStumpRegressor:\n",
    "    \"\"\"\n",
    "    Простое дерево глубины 1: выбирает признак и порог,\n",
    "    и предсказывает среднее значение в левом/правом узле.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_ = None\n",
    "        self.threshold_ = None\n",
    "        self.left_value_ = None\n",
    "        self.right_value_ = None\n",
    "\n",
    "    def fit(self, X, y, n_thresholds=40):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        n, d = X.shape\n",
    "\n",
    "        best_loss = np.inf\n",
    "        best = None\n",
    "\n",
    "        for f in range(d):\n",
    "            col = X[:, f]\n",
    "            uniq = np.unique(col)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "\n",
    "            # ограничим число порогов\n",
    "            if uniq.size > n_thresholds:\n",
    "                thr_list = np.quantile(uniq, np.linspace(0.05, 0.95, n_thresholds))\n",
    "            else:\n",
    "                thr_list = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "\n",
    "            for thr in thr_list:\n",
    "                mask = col <= thr\n",
    "                if mask.sum() == 0 or (~mask).sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                lv = y[mask].mean()\n",
    "                rv = y[~mask].mean()\n",
    "                pred = np.where(mask, lv, rv)\n",
    "                loss = np.mean((y - pred) ** 2)\n",
    "\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best = (f, float(thr), float(lv), float(rv))\n",
    "\n",
    "        if best is None:\n",
    "            # если ничего не нашли, всегда среднее\n",
    "            self.feature_ = 0\n",
    "            self.threshold_ = float(X[:, 0].mean())\n",
    "            m = float(y.mean())\n",
    "            self.left_value_ = m\n",
    "            self.right_value_ = m\n",
    "        else:\n",
    "            self.feature_, self.threshold_, self.left_value_, self.right_value_ = best\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        mask = X[:, self.feature_] <= self.threshold_\n",
    "        return np.where(mask, self.left_value_, self.right_value_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e5ac4-f1bd-4d8c-a991-e3e371366cab",
   "metadata": {},
   "source": [
    "## 7. Мой Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b596e9c-b385-4516-a85e-1d9fd281f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyGradientBoostingRegressor defined.\n"
     ]
    }
   ],
   "source": [
    "class MyGradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=80, learning_rate=0.1, random_state=42):\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.random_state = int(random_state)\n",
    "        self.base_ = 0.0\n",
    "        self.models_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "\n",
    "        self.base_ = float(y.mean())\n",
    "        pred = np.full_like(y, self.base_, dtype=float)\n",
    "\n",
    "        self.models_ = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - pred\n",
    "            stump = DecisionStumpRegressor().fit(X, residual)\n",
    "            update = stump.predict(X)\n",
    "            pred += self.learning_rate * update\n",
    "            self.models_.append(stump)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        pred = np.full(X.shape[0], self.base_, dtype=float)\n",
    "        for stump in self.models_:\n",
    "            pred += self.learning_rate * stump.predict(X)\n",
    "        return pred\n",
    "\n",
    "print(\"MyGradientBoostingRegressor defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b691ee-c62e-4232-9e71-01ea67ee913c",
   "metadata": {},
   "source": [
    "## 8. Мой Gradient Boosting Classifier (логистическая потеря)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d65212e-7f1a-4f89-93a2-b5be3f11b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyGradientBoostingClassifier defined.\n"
     ]
    }
   ],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "    \"\"\"\n",
    "    Упрощённый GB для бинарной классификации.\n",
    "    Моделируем F(x), p = sigmoid(F).\n",
    "    На каждом шаге обучаем stump на псевдоостатки: r = y - p.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=80, learning_rate=0.1, random_state=42):\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.random_state = int(random_state)\n",
    "        self.base_ = 0.0\n",
    "        self.models_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y).astype(float)\n",
    "\n",
    "        # начальное значение в логитах: log(p/(1-p))\n",
    "        p0 = np.clip(y.mean(), 1e-6, 1-1e-6)\n",
    "        self.base_ = float(np.log(p0 / (1 - p0)))\n",
    "\n",
    "        F = np.full_like(y, self.base_, dtype=float)\n",
    "\n",
    "        self.models_ = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            p = sigmoid(F)\n",
    "            residual = y - p  # псевдоостаток\n",
    "            stump = DecisionStumpRegressor().fit(X, residual)\n",
    "            F += self.learning_rate * stump.predict(X)\n",
    "            self.models_.append(stump)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        F = np.full(X.shape[0], self.base_, dtype=float)\n",
    "        for stump in self.models_:\n",
    "            F += self.learning_rate * stump.predict(X)\n",
    "        p1 = sigmoid(F)\n",
    "        p0 = 1.0 - p1\n",
    "        return np.vstack([p0, p1]).T\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X)[:, 1] >= threshold).astype(int)\n",
    "\n",
    "print(\"MyGradientBoostingClassifier defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5954b3b6-6d64-49b9-a8e2-42b1431cff35",
   "metadata": {},
   "source": [
    "## 9. Обучение/оценка самописных моделей (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "624fc117-4736-4430-b4a8-591419254aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyGB baseline (classification): {'accuracy': 0.7656, 'precision': 0.726790070311379, 'recall': 0.9200726612170754, 'f1': 0.8120891454224788, 'roc_auc': np.float64(0.8776631821208144)}\n",
      "MyGB baseline (regression):    {'rmse': 21.4105445882524, 'mae': 16.990754455437628, 'r2': 0.049090283136535295}\n"
     ]
    }
   ],
   "source": [
    "#  baseline \n",
    "my_gb_cls_base = MyGradientBoostingClassifier(n_estimators=60, learning_rate=0.1, random_state=RANDOM_STATE)\n",
    "my_gb_cls_base.fit(Xc_tr_s, yc_tr_s)\n",
    "\n",
    "yc_pred_m = my_gb_cls_base.predict(Xc_te_arr)\n",
    "yc_proba_m = my_gb_cls_base.predict_proba(Xc_te_arr)[:, 1]\n",
    "\n",
    "my_baseline_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_te, yc_pred_m),\n",
    "    \"precision\": precision_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"recall\": recall_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"f1\": f1_score(yc_te, yc_pred_m, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_te, yc_proba_m),\n",
    "}\n",
    "print(\"MyGB baseline (classification):\", my_baseline_cls_res)\n",
    "\n",
    "\n",
    "my_gb_reg_base = MyGradientBoostingRegressor(n_estimators=60, learning_rate=0.1, random_state=RANDOM_STATE)\n",
    "my_gb_reg_base.fit(Xr_tr_s, yr_tr_s)\n",
    "\n",
    "yr_pred_m = my_gb_reg_base.predict(Xr_te_arr)\n",
    "\n",
    "my_baseline_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_te, yr_pred_m),\n",
    "    \"mae\": mean_absolute_error(yr_te, yr_pred_m),\n",
    "    \"r2\": r2_score(yr_te, yr_pred_m),\n",
    "}\n",
    "print(\"MyGB baseline (regression):   \", my_baseline_reg_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ec26e-e660-41d2-9996-c5a280e4413f",
   "metadata": {},
   "source": [
    "## 10. Улучшенный самописный вариант\n",
    "Идея: больше итераций + меньше learning_rate (обычно стабильнее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464abbf9-49a5-4477-8ce7-579465eb7c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyGB improved (classification): {'accuracy': 0.8015, 'precision': 0.7685382972230699, 'recall': 0.9149863760217983, 'f1': 0.8353926527904469, 'roc_auc': np.float64(0.8823811097000502)}\n",
      "MyGB improved (regression):    {'rmse': 21.3768871125267, 'mae': 16.965951072722532, 'r2': 0.0520776022495919}\n"
     ]
    }
   ],
   "source": [
    "my_gb_cls_imp = MyGradientBoostingClassifier(n_estimators=150, learning_rate=0.05, random_state=RANDOM_STATE)\n",
    "my_gb_cls_imp.fit(Xc_tr_s, yc_tr_s)\n",
    "\n",
    "yc_pred_m2 = my_gb_cls_imp.predict(Xc_te_arr)\n",
    "yc_proba_m2 = my_gb_cls_imp.predict_proba(Xc_te_arr)[:, 1]\n",
    "\n",
    "my_improved_cls_res = {\n",
    "    \"accuracy\": accuracy_score(yc_te, yc_pred_m2),\n",
    "    \"precision\": precision_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"recall\": recall_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"f1\": f1_score(yc_te, yc_pred_m2, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(yc_te, yc_proba_m2),\n",
    "}\n",
    "print(\"MyGB improved (classification):\", my_improved_cls_res)\n",
    "\n",
    "\n",
    "my_gb_reg_imp = MyGradientBoostingRegressor(n_estimators=150, learning_rate=0.05, random_state=RANDOM_STATE)\n",
    "my_gb_reg_imp.fit(Xr_tr_s, yr_tr_s)\n",
    "\n",
    "yr_pred_m2 = my_gb_reg_imp.predict(Xr_te_arr)\n",
    "\n",
    "my_improved_reg_res = {\n",
    "    \"rmse\": root_mean_squared_error(yr_te, yr_pred_m2),\n",
    "    \"mae\": mean_absolute_error(yr_te, yr_pred_m2),\n",
    "    \"r2\": r2_score(yr_te, yr_pred_m2),\n",
    "}\n",
    "print(\"MyGB improved (regression):   \", my_improved_reg_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93576197",
   "metadata": {},
   "source": [
    "## 11. Сравнение результатов и выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b505bc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sk_baseline</th>\n",
       "      <td>0.9194</td>\n",
       "      <td>0.913135</td>\n",
       "      <td>0.943324</td>\n",
       "      <td>0.927984</td>\n",
       "      <td>0.979734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_improved</th>\n",
       "      <td>0.9278</td>\n",
       "      <td>0.926216</td>\n",
       "      <td>0.944051</td>\n",
       "      <td>0.935049</td>\n",
       "      <td>0.984284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_baseline</th>\n",
       "      <td>0.7656</td>\n",
       "      <td>0.726790</td>\n",
       "      <td>0.920073</td>\n",
       "      <td>0.812089</td>\n",
       "      <td>0.877663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_improved</th>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.768538</td>\n",
       "      <td>0.914986</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.882381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  precision    recall        f1   roc_auc\n",
       "sk_baseline    0.9194   0.913135  0.943324  0.927984  0.979734\n",
       "sk_improved    0.9278   0.926216  0.944051  0.935049  0.984284\n",
       "my_baseline    0.7656   0.726790  0.920073  0.812089  0.877663\n",
       "my_improved    0.8015   0.768538  0.914986  0.835393  0.882381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sk_baseline</th>\n",
       "      <td>20.705978</td>\n",
       "      <td>16.414578</td>\n",
       "      <td>0.110645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk_improved</th>\n",
       "      <td>19.972980</td>\n",
       "      <td>15.856034</td>\n",
       "      <td>0.172497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_baseline</th>\n",
       "      <td>21.410545</td>\n",
       "      <td>16.990754</td>\n",
       "      <td>0.049090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_improved</th>\n",
       "      <td>21.376887</td>\n",
       "      <td>16.965951</td>\n",
       "      <td>0.052078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rmse        mae        r2\n",
       "sk_baseline  20.705978  16.414578  0.110645\n",
       "sk_improved  19.972980  15.856034  0.172497\n",
       "my_baseline  21.410545  16.990754  0.049090\n",
       "my_improved  21.376887  16.965951  0.052078"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_compare = pd.DataFrame(\n",
    "    [baseline_cls_res, improved_cls_res, my_baseline_cls_res, my_improved_cls_res],\n",
    "    index=[\"sk_baseline\", \"sk_improved\", \"my_baseline\", \"my_improved\"]\n",
    ")\n",
    "\n",
    "reg_compare = pd.DataFrame(\n",
    "    [baseline_reg_res, improved_reg_res, my_baseline_reg_res, my_improved_reg_res],\n",
    "    index=[\"sk_baseline\", \"sk_improved\", \"my_baseline\", \"my_improved\"]\n",
    ")\n",
    "\n",
    "display(cls_compare)\n",
    "display(reg_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8e8f6",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "В данной лабораторной работе были исследованы модели градиентного бустинга для задач классификации и регрессии, включая библиотечные и самописные реализации.\n",
    "\n",
    "### Классификация (loan_status)\n",
    "\n",
    "Градиентный бустинг показал очень высокое качество классификации. Уже бейзлайновая модель `sk_baseline` демонстрирует отличные результаты, а подбор гиперпараметров позволил дополнительно улучшить качество:\n",
    "- accuracy увеличилась с **0.919 → 0.928**;\n",
    "- F1-score вырос с **0.928 → 0.935**;\n",
    "- ROC-AUC достиг значения **0.984**, что говорит о высокой способности модели различать классы.\n",
    "\n",
    "Самописная реализация градиентного бустинга работает корректно, однако значительно уступает библиотечной версии. Несмотря на улучшение показателей у `my_improved`, значения accuracy и F1-score остаются заметно ниже, что объясняется упрощённой реализацией и отсутствием продвинутых оптимизаций.\n",
    "\n",
    "### Регрессия (song_popularity)\n",
    "\n",
    "Для задачи регрессии библиотечная модель градиентного бустинга показала умеренное улучшение качества после настройки гиперпараметров:\n",
    "- RMSE снизилась с **20.71 → 19.97**;\n",
    "- коэффициент детерминации **R²** вырос с **0.11 → 0.17**.\n",
    "\n",
    "Самописные модели регрессии демонстрируют более слабые результаты. Значения R² остаются близкими к нулю, что указывает на низкую точность аппроксимации. Улучшение самописной модели дало лишь незначительный эффект.\n",
    "\n",
    "### Итог\n",
    "\n",
    "Градиентный бустинг является одним из наиболее эффективных алгоритмов для задачи классификации и показывает устойчивое улучшение качества при настройке гиперпараметров. В задаче регрессии он превосходит линейные модели и решающие деревья, однако остаётся чувствительным к настройкам. Библиотечные реализации `sklearn` значительно превосходят самописные модели по качеству и стабильности и предпочтительны для практического применения, тогда как самописные реализации носят учебный характер.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63944de",
   "metadata": {},
   "source": [
    "# Итоговое сравнение результатов лабораторных работ №1–5\n",
    "\n",
    "В ходе выполнения лабораторных работ были последовательно исследованы пять алгоритмов машинного обучения: **KNN**, **линейные модели (логистическая и линейная регрессия)**, **решающее дерево**, **случайный лес** и **градиентный бустинг**. Для каждого алгоритма рассматривались задачи классификации и регрессии, а также сравнивались библиотечные и самописные реализации.\n",
    "\n",
    "### Сравнение алгоритмов в задаче классификации\n",
    "\n",
    "- **KNN (ЛР1)** показал хорошие результаты после подбора гиперпараметров, однако чувствительность к выбору `k` и высокая вычислительная сложность ограничивают его применение на больших выборках.\n",
    "- **Логистическая регрессия (ЛР2)** продемонстрировала стабильное, но умеренное качество. Улучшение за счёт настройки регуляризации оказалось незначительным.\n",
    "- **Решающее дерево (ЛР3)** обеспечило заметный прирост качества по сравнению с линейными моделями, но оказалось склонным к переобучению.\n",
    "- **Случайный лес (ЛР4)** показал одно из лучших качеств классификации: высокие значения accuracy, F1 и ROC-AUC, при этом модель оказалась устойчивой и стабильной.\n",
    "- **Градиентный бустинг (ЛР5)** стал лучшим алгоритмом для классификации, обеспечив максимальные значения ROC-AUC и F1-score. Подбор гиперпараметров дал дополнительный, пусть и умеренный, прирост качества.\n",
    "\n",
    "**Итог по классификации:**  \n",
    "Наилучшие результаты показали ансамблевые методы — сначала случайный лес, а затем градиентный бустинг, который стал лидером по качеству.\n",
    "\n",
    "### Сравнение алгоритмов в задаче регрессии\n",
    "\n",
    "- **KNN-регрессия (ЛР1)** улучшилась после подбора гиперпараметров, но осталась чувствительной к шуму и масштабированию данных.\n",
    "- **Линейная регрессия (ЛР2)** показала низкие значения R², что указывает на слабую линейную зависимость между признаками и целевой переменной.\n",
    "- **Решающее дерево (ЛР3)** дало нестабильные результаты и часто показывало низкий или отрицательный R².\n",
    "- **Случайный лес (ЛР4)** обеспечил наилучшее качество регрессии среди всех алгоритмов, снизив RMSE и повысив R².\n",
    "- **Градиентный бустинг (ЛР5)** показал улучшение по сравнению с деревом и линейными моделями, однако в данной задаче уступил случайному лесу.\n",
    "\n",
    "**Итог по регрессии:**  \n",
    "Наиболее эффективным алгоритмом оказался случайный лес, обеспечивший лучшее соотношение точности и устойчивости.\n",
    "\n",
    "### Сравнение библиотечных и самописных реализаций\n",
    "\n",
    "Во всех лабораторных работах самописные модели:\n",
    "- корректно воспроизводили базовую логику алгоритмов;\n",
    "- демонстрировали сопоставимые, но более низкие результаты;\n",
    "- уступали библиотечным реализациям по качеству и стабильности.\n",
    "\n",
    "Это объясняется отсутствием оптимизаций, более простой реализацией и ограничениями по вычислительным ресурсам.\n",
    "\n",
    "### Общий вывод\n",
    "\n",
    "Ансамблевые методы (случайный лес и градиентный бустинг) показали наилучшие результаты как в классификации, так и в регрессии. Подбор гиперпараметров во всех случаях приводил к улучшению качества, особенно для сложных моделей. Библиотечные реализации `sklearn` являются предпочтительными для практического применения, тогда как самописные модели целесообразно рассматривать как учебный инструмент для понимания принципов работы алгоритмов машинного обучения.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
